# BunCLI-Kit üöÄ

[![en](https://img.shields.io/badge/lang-en-red.svg)](README.md)

Un kit de d√©veloppement CLI TypeScript puissant et moderne propuls√© par Bun, con√ßu pour cr√©er des applications en ligne de commande robustes avec facilit√©. Cette bo√Æte √† outils fournit une mani√®re propre et structur√©e de cr√©er des commandes CLI en utilisant TypeScript, Zod pour la validation, et Bun pour une ex√©cution rapide. Elle int√®gre un syst√®me complet d'IA qui supporte plusieurs fournisseurs de mod√®les (OpenAI, Anthropic, Ollama) permettant d'enrichir vos commandes CLI avec des capacit√©s d'intelligence artificielle de mani√®re simple et efficace.

## üåü Fonctionnalit√©s

- **TypeScript First**: Construit avec TypeScript pour une s√©curit√© de type maximale et une exp√©rience d√©veloppeur optimale
- **Propuls√© par Bun**: Exploite la vitesse et les fonctionnalit√©s modernes de Bun
- **Architecture Propre**: Impl√©mente l'architecture hexagonale avec les principes du domain-driven design
- **Validation des Donn√©es**: Validation des sch√©mas Zod int√©gr√©e pour une gestion robuste des commandes
- **Support Multi-IA**: Int√©gration native avec OpenAI (GPT), Anthropic (Claude) et Ollama (mod√®les open source)
- **Exp√©rience D√©veloppeur**: Inclut la configuration ESLint et Prettier pr√™te √† l'emploi
- **S√©curit√© des Types**: Configuration TypeScript stricte pour un code fiable
- **Patterns Modernes**: Impl√©mente les principes SOLID et les pratiques de code propre
- **Logging Avanc√©**: Syst√®me de logging flexible avec plusieurs options de sortie

## üîë Configuration des Variables d'Environnement

BunCLI-Kit utilise des variables d'environnement pour g√©rer les configurations sensibles comme les cl√©s API. Un fichier `.env.dist` est fourni comme mod√®le. Pour configurer votre environnement :

1. Copiez le fichier mod√®le :
```bash
cp .env.dist .env
```

2. Modifiez le fichier `.env` avec vos configurations :

```env
# Configuration OpenAI
OPENAI_API_KEY=votre-cl√©-api-openai
OPENAI_BASE_URL=https://api.openai.com/v1  # Optionnel, peut √™tre modifi√© pour utiliser d'autres services compatibles

# Configuration Anthropic
ANTHROPIC_API_KEY=votre-cl√©-api-anthropic

# Configuration Ollama
OLLAMA_BASE_URL=http://localhost:11434  # Optionnel, modifiez si Ollama est sur un autre h√¥te
```

### Services Compatibles OpenAI

Le syst√®me supporte diff√©rents services compatibles avec l'API OpenAI. Voici quelques exemples de configuration :

```env
# OpenAI standard
OPENAI_BASE_URL=https://api.openai.com/v1

# Azure OpenAI
OPENAI_BASE_URL=https://votre-ressource.openai.azure.com

# LocalAI
OPENAI_BASE_URL=http://localhost:8080/v1

# Autres services compatibles
OPENAI_BASE_URL=https://api.votre-service.com/v1
```

‚ö†Ô∏è **Important** : 
- Le fichier `.env` est ignor√© par Git pour prot√©ger vos informations sensibles
- Ne committez jamais vos vraies cl√©s API dans le d√©p√¥t
- Gardez une copie s√©curis√©e de vos cl√©s API

## üìù Syst√®me de Logging

Le BunCLI-Kit inclut un puissant syst√®me de logging via le `LoggerService` qui vous aide √† suivre et d√©boguer votre application :

- **Sortie Flexible**: Support pour la console et le logging dans des fichiers
- **Niveaux de Log**: Diff√©rents niveaux de log (INFO, ERROR, DEBUG, etc.)
- **Interface Propre**: Impl√©mentation de l'interface `LoggerPort` pour une extension facile
- **Injection de D√©pendances**: Suit les principes de l'architecture propre

Exemple d'utilisation :

```typescript
// Injecter le service de logging
constructor(private readonly logger: LoggerPort) {}

// Utiliser dans votre code
this.logger.info('Commande ex√©cut√©e avec succ√®s');
this.logger.error('Une erreur est survenue', error);
this.logger.debug('Information de d√©bogage');
```

## üöÄ D√©marrage Rapide

```bash
# Cloner le d√©p√¥t
git clone https://github.com/offroadlabs/buncli-kit.git
cd buncli-kit

# Installer les d√©pendances
bun install

# Voir les commandes disponibles
bun run help

# Cr√©er une nouvelle commande CLI
bun run command:create <nom-commande>

# Supprimer une commande CLI
bun run command:remove <nom-commande>
```

## üìñ Cr√©ation de Nouvelles Commandes

1. Cr√©ez une nouvelle commande en utilisant le g√©n√©rateur :

```bash
bun run command:create ma-commande
```

2. Cela va automatiquement :
   - Cr√©er un nouveau fichier de commande dans `src/infrastructure/commands/MaCommandeCommand.ts`
   - Mettre √† jour `src/index.ts` pour enregistrer la commande
   - Ajouter un script √† `package.json`

La commande g√©n√©r√©e aura cette structure :

```typescript
import { CommandPort } from "../../domain/ports/CommandPort";

export class MaCommandeCommand implements CommandPort {
    getName(): string {
        return 'ma-commande';
    }

    getDescription(): string {
        return 'Description de la commande ma-commande';
    }

    async execute(args: string[]): Promise<void> {
        // Impl√©mentez votre logique de commande ici
        console.log('ma-commande ex√©cut√©e');
    }
}
```

## üóëÔ∏è Suppression de Commandes

Pour supprimer une commande de votre CLI :

```bash
bun run command:remove ma-commande
```

Cela va :
- Supprimer le fichier de commande
- Nettoyer les imports dans `src/index.ts`
- Supprimer le script de `package.json`

## üõ†Ô∏è Directives de D√©veloppement

- Utilisez les sch√©mas Zod pour la validation des arguments de commande
- Suivez le mod√®le d'architecture hexagonale :
  - `domain/`: Logique m√©tier et interfaces
  - `infrastructure/`: Impl√©mentations des commandes
  - `application/`: Services d'application
- √âcrivez du code propre et maintenable suivant les principes SOLID
- Utilisez la configuration ESLint et Prettier fournie
- Ajoutez des tests pour vos commandes en utilisant le runner de test de Bun

## üé® Style de Code & Linting

Ce projet utilise ESLint et Prettier pour assurer un style de code coh√©rent et d√©tecter les probl√®mes potentiels t√¥t. La configuration est con√ßue pour appliquer les meilleures pratiques TypeScript et maintenir une haute qualit√© de code.

### Configuration ESLint

Le projet utilise une configuration moderne plate (`eslint.config.js`) avec des r√®gles TypeScript strictes :

```javascript
// R√®gles ESLint principales :
{
  '@typescript-eslint/explicit-function-return-type': 'error',
  '@typescript-eslint/no-explicit-any': 'error',
  '@typescript-eslint/strict-boolean-expressions': 'error',
  '@typescript-eslint/no-floating-promises': 'error',
  '@typescript-eslint/no-misused-promises': 'error',
  'eqeqeq': 'error',
  'no-var': 'error',
  'prefer-const': 'error'
}
```

### Configuration Prettier

Le formatage du code est g√©r√© par Prettier avec les param√®tres suivants :

```json
{
  "semi": true,
  "trailingComma": "es5",
  "singleQuote": true,
  "printWidth": 100,
  "tabWidth": 2,
  "useTabs": false,
  "bracketSpacing": true,
  "arrowParens": "avoid"
}
```

### Ex√©cution du Linting

```bash
# Lancer ESLint
bun run lint

# Corriger les probl√®mes auto-fixables
bun run lint --fix
```

## ü§ñ Intelligence Artificielle

BunCLI-Kit int√®gre un syst√®me flexible pour interagir avec diff√©rents mod√®les d'IA via une architecture propre et extensible.

### Architecture IA

- **AiModelService**: Service central g√©rant les interactions et la validation des mod√®les d'IA
- **Interface IAiModel**: Interface de base pour tous les mod√®les d'IA
- **Formatters**: Syst√®me de formatage pour parser les r√©ponses de l'IA
- **Factory Pattern**: Cr√©ation de mod√®les d'IA via le singleton `AiModelFactory`
- **Support Streaming**: Capacit√©s de streaming int√©gr√©es pour les r√©ponses d'IA

### Mod√®les d'IA Support√©s

BunCLI-Kit prend en charge plusieurs fournisseurs de mod√®les d'IA :

#### OpenAI
- Mod√®les GPT (3.5-turbo, GPT-4, etc.)
- Support complet du streaming
- Compatible avec les services Azure OpenAI et autres API compatibles OpenAI
- Configuration via `OPENAI_API_KEY` et `OPENAI_BASE_URL`

#### Anthropic
- Mod√®les Claude (Claude 3 Opus, Sonnet, Haiku)
- Support du streaming
- Configuration via `ANTHROPIC_API_KEY`

#### Ollama (Local)
- Mod√®les open source (Mistral, Llama, CodeLlama, etc.)
- Ex√©cution locale des mod√®les
- Support du streaming
- Configuration via `OLLAMA_BASE_URL`

Exemple d'utilisation avec diff√©rents mod√®les :

```typescript
// Utilisation d'OpenAI
const openaiModel = this.aiModelService.createModel('openai', 'gpt-4');

// Utilisation d'Anthropic
const anthropicModel = this.aiModelService.createModel('anthropic', 'claude-3-opus-20240229');

// Utilisation d'Ollama
const ollamaModel = this.aiModelService.createModel('ollama', 'mistral');
```

### Utilisation du AiModelService

Le syst√®me d'IA fournit un service centralis√© pour la gestion des mod√®les d'IA. Voici un exemple complet :

```typescript
import { CommandPort } from '@/domain/ports/CommandPort';
import { LoggerService } from '@/application/services/LoggerService';
import { AiModelService } from '@/application/services/AiModelService';
import { z } from 'zod';

// D√©finir vos sch√©mas
const WeatherDataSchema = z.object({
  temperature: z.number(),
  conditions: z.string(),
  location: z.string(),
});

export class MyAiCommand implements CommandPort {
  private readonly logger;
  private readonly aiModelService;

  constructor() {
    this.logger = LoggerService.getInstance().getLogger({
      prefix: 'my-ai-command',
      timestamp: false,
    });
    this.aiModelService = AiModelService.getInstance();
  }

  async execute(): Promise<void> {
    const model = this.aiModelService.createModel('ollama', 'mistral');

    try {
      // Exemple avec validation de sch√©ma
      const response = await model.generate(
        'Give me the weather in Paris. For temperature, write 9 for 9¬∞C, 10 for 10¬∞C, etc.',
        {
          temperature: 0.7,
          systemPrompt: 'You are a weather reporter. Write in Spanish.',
          schema: WeatherDataSchema,
        }
      );

      // Valider la r√©ponse avec AiModelService
      const isValid = await this.aiModelService.validateModelResponse(
        response.content,
        WeatherDataSchema
      );

      if (isValid) {
        this.logger.info('Donn√©es m√©t√©o :');
        this.logger.info('- Temp√©rature :', response.content?.temperature ?? 'N/A');
        this.logger.info('- Conditions :', response.content?.conditions ?? 'N/A');
        this.logger.info('- Location :', response.content?.location ?? 'N/A');
      } else {
        this.logger.error('Format de r√©ponse invalide');
      }

      // Exemple avec streaming et transformation
      this.logger.info('\nR√©ponse en streaming avec transformation :');
      const upperCaseFormatter = (content: string): string => content.toUpperCase();

      if (model.streamGenerate) {
        for await (const chunk of model.streamGenerate<string>('Tell me a short story.', {
          temperature: 0.7,
          formatter: upperCaseFormatter,
          systemPrompt: 'in french.',
        })) {
          process.stdout.write(chunk.content ?? 'N/A');
        }
      }
    } catch (error) {
      this.logger.error('Erreur :', error);
    }
  }
}
```

### Fonctionnalit√©s Cl√©s

- Gestion centralis√©e des mod√®les d'IA via AiModelService
- Typage fort avec sch√©mas Zod pour les r√©ponses de l'IA
- Validation int√©gr√©e des r√©ponses
- Support de plusieurs types de mod√®les d'IA
- Support du streaming avec transformation
- Configuration flexible (temp√©rature, prompt syst√®me)
- Gestion compl√®te des logs et des erreurs

### Utilisation des Formatters Personnalis√©s

En plus de la validation des sch√©mas, vous pouvez utiliser des formatters personnalis√©s pour transformer les r√©ponses. Voici comment les utiliser avec AiModelService :

```typescript
import { CommandPort } from '@/domain/ports/CommandPort';
import { LoggerService } from '@/application/services/LoggerService';
import { AiModelService } from '@/application/services/AiModelService';

export class MyFormatterCommand implements CommandPort {
  private readonly logger;
  private readonly aiModelService;

  constructor() {
    this.logger = LoggerService.getInstance().getLogger({
      prefix: 'my-formatter-command',
      timestamp: false,
    });
    this.aiModelService = AiModelService.getInstance();
  }

  async execute(): Promise<void> {
    const model = this.aiModelService.createModel('ollama', 'mistral');

    try {
      // Exemple avec un formatter simple qui met le texte en majuscules
      const upperCaseFormatter = (content: string): string => content.toUpperCase();
      
      const response = await model.generate(
        'Raconte-moi une courte histoire.',
        {
          temperature: 0.7,
          formatter: upperCaseFormatter,
          systemPrompt: 'Tu es un conteur d\'histoires.',
        }
      );

      this.logger.info('Histoire en majuscules :', response.content);

      // Exemple avec un formatter qui ajoute un pr√©fixe et un suffixe
      const wrapFormatter = (content: string): string => {
        return `üåü ${content} üåü`;
      };

      const wrappedResponse = await model.generate(
        'Donne-moi une citation inspirante.',
        {
          temperature: 0.7,
          formatter: wrapFormatter,
          systemPrompt: 'Tu es un coach motivant.',
        }
      );

      this.logger.info('Citation d√©cor√©e :', wrappedResponse.content);

      // Exemple avec streaming et transformation
      this.logger.info('\nR√©ponse en streaming avec transformation :');
      
      if (model.streamGenerate) {
        for await (const chunk of model.streamGenerate<string>(
          'Raconte-moi une blague.',
          {
            temperature: 0.7,
            formatter: upperCaseFormatter,
            systemPrompt: 'Tu es un com√©dien.',
          }
        )) {
          process.stdout.write(chunk.content ?? 'N/A');
        }
      }
    } catch (error) {
      this.logger.error('Erreur :', error);
    }
  }
}
```

Les formatters peuvent √™tre utilis√©s pour :
- Transformer le texte (majuscules, minuscules, etc.)
- Ajouter des d√©corations ou du formatage
- Nettoyer ou normaliser les r√©ponses
- Appliquer des transformations personnalis√©es
- Traiter les r√©ponses avant la validation

## üîß Services Professionnels

### Expertise Technique

Je propose des services de d√©veloppement et de conseil dans les domaines suivants :

- Applications Web Modernes (Next.js, React, TypeScript)
- APIs et Microservices (Symfony, Node.js)
- Architecture Logicielle et DevOps
- Formation et Support Technique

### Domaines d'Intervention

- D√©veloppement d'Applications Sur Mesure
- Migration et Modernisation de Syst√®mes Legacy
- Optimisation des Performances
- Conseil Technique

### Technologies Ma√Ætris√©es

- **Frontend**: TypeScript, React, Next.js, Tailwind
- **Backend**: PHP/Symfony, Node.js
- **Mobile**: Flutter, React Native
- **DevOps**: Docker, CI/CD, AWS
- **Bases de donn√©es**: PostgreSQL, MySQL, MongoDB

## üì´ Contact

Pour toute demande de collaboration ou de d√©veloppement sur mesure :

- üìß Email: [sebastien@offroadlabs.com](mailto:sebastien@offroadlabs.com)
- üìù Blog: [https://timoner.com](https://timoner.com)
- üåê Site Web: [https://offroadlabs.com](https://offroadlabs.com)
- üìÖ Calendrier: [Planifier un rendez-vous](https://hub.timoner.com)
- üìç Localisation: Aix-en-Provence, France

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## ‚≠ê Contribuer

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou √† soumettre une pull request.

---

D√©velopp√© par [S√©bastien TIMONER](https://github.com/offroadlabs) 